# FIAP - Faculdade de Inform√°tica e Administra√ß√£o Paulista 

![FIAP](assets/logo-fiap.png)

# Fase 05 Cap 1

## FarmTech na era da cloud computing

## Grupo DRELL

## üë®‚Äçüéì Integrantes: 
- <a href="https://www.linkedin.com/in/douglas-souza-felipe-b815281a2/">Douglas</a>
- <a href="https://www.linkedin.com/in/richard-marques-26b3a14/">Richard</a>
- <a href="https://www.linkedin.com/in/lucasmedeirosleite">Lucas Medeiros</a> 
- <a href="https://www.linkedin.com/in/evelyn-z-342a07365/">Evelyn Z</a> 
- <a href="https://www.linkedin.com/in/luis-fernando-dos-santos-costa-b69894365/">Luis</a>

## üë©‚Äçüè´ Professores:
### Tutor(a) 
- <a href="https://github.com/leoruiz197">Leo Ruiz</a>
### Coordenador(a)
- <a href="https://www.linkedin.com/in/andregodoichiovato/">Andre Godoi</a>


## üìú Descri√ß√£o

Este reposit√≥rio cont√©m os assets usados para explorar a base de dados de cultivo fornecida pelo professor para poder treinar modelos de IA supervisionado e n√£o supervisionado para identificar os itens a seguir:
- Encontrar tendencias para os rendimentos das planta√ß√µes usando modelos n√£o supervisionados para agrupar (clusteriza√ß√£o) os dados da base permitindo inclusive identificar os "pontos fora da curva" chamados outliers.
- Prever o rendimento da safra usando modelos supervisionados de predi√ß√£o (testar 5 algoritmos diferentes comprando sua performance com indicadores com acur√°cia, precis√£o e F1-Score)

Todo o projeto explicado neste tutorial esta detalhado (usando a mesma estrutura de cap√≠ulos no Jupyter Notebook na pasta assets deste reposit√≥rio: [Codigo](assets/RichardMarques_rm563313_pbl_fase5.ipynb)

### Pre-requisitos

Para executar os c√≥digos deste repost√≥rio voc√™ vai precisar atender os seguintes requisitos t√©cnicos:
- Python 3.9 ou superior
- ter instalado as bibliotecas contidas no arquivo requirements.txt instaladas no seu ambiente

```
python3 -m pip install -r requirements.txt
```

Para cupriir estes objetivos o trabalho foi realizado usando a seguinte metodologia:

### 1 - Explora√ß√£o, entendimento e prepara√ß√£o dos dados
- Verificar a qualidade dos dados (se o modelo √© bem comportado ou n√£o virificando duplicadade, valores nulos e distribui√ß√£o dos dados)
- Preparar os dados (usando PCA para reduzir a quantidade de dimens√µes se necess√°rio, remo√ß√£o de valores nulos e/ou outliers, e removendo registros duplicados)

#### 1.1 - Conhecendo os dados
Primeira etapa foi carregar o modelo e tentar entender suas caracter√≠sticas, distribui√ß√£o e features. Para isso usamos a biblioteca "Pandas" para carregar os dados e usar o m√©todo "info()" para conseguir mais informa√ß√µes sobre ele.

O resultado foi a lista das features como no exmeplo a seguir:
![Metodo Info](assets/1_1_a.png)

E tamb√©m ainda usando o pandas, como m√©todo "head()" listamos os primeiros registros desta base. 
![Primeiros Registros](assets/1_1_b.png)

Chegamos a conclus√£o que esta √© uma base bem comportada, n√£o possui campos nulos. 


#### 1.2 - Nornaliznado os dados
Pr√≥ximo passo, como descobrimos na etapa anterior, temos um campo texto, chamado "Crop" que representa o tipo de cultivo. Como aprendemos os modelos supervisionados e n√£o supervisionados n√£o lidam bem com dados que n√£o s√£o num√©ricos. Por isso, vamos converter esses texto para um valor num√©rico.

Aqui vamos inserir no nosso projeto uma nova biblioteca, chamada "sklearn" que possuem v√°rias fun√ß√µes para trabalhar e manipular dados. Neste caso vamos usar o "LabelEncoder()" que faz justamente a convers√£o de valores texto para alguma representa√ß√£o num√©rica usando o m√©todo "fit_transformer" passando a coluna que queremos transformar como par√¢metro, no caso a "Crop".

Desta forma o campo Crop foi convertido para um sequencial onde:
- 0 = Cocoa, beans
- 1 = Oil palm fruit
- 2 = Rice, paddy
- 3 = Rubber, natural

#### 1.3 - Duplicados
Pr√≥ximo passo √© verificar se existem dados duplicados na nossa base e remove-los. Para isso o pr√≥prio objeto DataSet que presenta a nossa base, gerado a partir do carregamento da base com o Pandas, possui um m√©todo para fazer esta valida√ß√£o. Usando o c√≥digo a seguir conseguimos verificar se existem itens duplicados e remove-los se for verdade. 

```
# Verificar duplicados
duplicados = df[df.duplicated()]

# Resultado
if duplicados.empty:
    print("‚úÖ N√£o existem registros duplicados na base.")
else:
    print("‚ö†Ô∏è Existem registros duplicados. Removendo dados!")
    df.drop_duplicates(inplace=True)
```

#### 1.4 - Avaliando a import√¢ncia de cada feature 
Pr√≥xima etapa √© avaliar o peso que cada feature no resultado final da produ√ß√£o. Para isso escolhi o algoritmo "RandomForestClassifier" para treinar e testar um modelo de classifica√ß√£o. Esse algoritmo esta dispon√≠vel tamb√©m na biblioteca "sklearn". Uma vez treinado o modelo, ele possui um m√©todo que nos traz esta informa√ß√£o, o m√©todo chamado "feature_importances_". Segue abaixo o resultado conseguido com a nossa base depois das etapas de remova√ß√£o de duplica√ß√£o e encode. 

![Detalhes da import√¢ncia das Features](assets/1_4_a.png)

Agora usamos a biblioteca "matplotlib" junto com a "seaborn" para exibir um gr√°fico mostrando esses dados. 

![Gr√°fico da import√¢ncia das features](assets/1_4_b.png)

#### 1.5 - Cmoparando a rela√ß√£o entre as features
Agora vamos entender a influ√™ncia e rela√ß√£o de cada feature comparada com as demais. Novamente vamos usar a combina√ß√£o das bibliotecas "matplotlib" e "seaborn" para fazer a exibi√ß√£o dessa informma√ß√£o entre a rela√ß√£o e a distribui√ß√£o dos dados quando comparada as suas features. 

![Rela√ß√£o entre as features](assets/1_5_a.png)

Vamos ver essa rela√ß√£o por outra perspectiva, com uma nota de influ√™ncia, quanto mais pr√≥ximo de 1 mais as vari√°veis est√£o relacionadas. 

![Score de rela√ß√£o](assets/1_5_b.png]

#### 1.6 - Removendo OutLieers
Para melhor separarmos os clusters, vamos identificar e remover os "pontos fora da curva", conhecidos pelo termo em ingles "ouliers". 
Para isso vamos usar o algoritmo "IsolationFerest" da biblioteca "skylearn" que √© recomendado para dados multimensionais como as nossas features clim√°ticas. 

#### 1.7 - Conclus√£o sobre a explora√ß√£o dos dados
Nesta an√°lise inicial que fizemos descobrimos que as features mais importantes s√£o o tipo de cultivo e a precipta√ß√£o de √°gua. Vimos que os dados est√£o bem coportados por n√£o possuem nulos e tamb√©m n√£o encontramos registros duplicados. Aplicamos um encode para facilitar o trabalho com a coluna "Crop" originalmente em texto. 
Em resumo:
- base pequena, com poucas amostras, 156 no total
- poucos campos, apenas 6 feautres 
- o campo Crop representa a categoria / tipo de cultivo
- os campos Precipitation (mm day-1),Specific Humidity at 2 Meters (g/kg),Relative Humidity at 2 Meters (%),Temperature at 2 Meters (C) representam as condi√ß√µes clim√°ticas
- o campo "Yield" representa a produ√ß√£o daquela safra dadas o tipo de cultivo e condi√ß√µes clim√°ticas


### 2 - Classificar os dados com modelo n√£o supervisionado
- identificar qual o melhor algoritimo para esta tarefa dado a miss√£o e caracter√≠stica dos dados (remova√ß√£o de bias, variedade dos dados, etc)
- Definir a melhor parametrixa√ß√£o do modelo dado o objetivo (quantidade de clusters)
- separar dados para testes e treinamento
- treinar o modelo
- validar seu resultado

#### 2.1 - Avaliar o melhor modelo para esta tarefa
Dado a exploca√ß√£o que fizemos nos dados e as conclus√µes que chegamos no item 1.7 e que nossa miss√£o √© encontrar grupos claros e compactos de culturas com base nas condi√ß√µes clim√°ticas entendo que o KMeans √© a melhor op√ß√£o por funcionar bem com datasets menores. O DBScan por exemplo tem melhores resultados com mais exemplos (base de dados maior).

#### 2.2 - Normalizar os dados originalmente em escalas diferentes
Para facilitar o estudo precisamos colocar os diferentes campos em uma mesma escala para entender melhor a distribui√ß√£o dos dados. Para esta tarefa vamos usar o "StandardScaler" que tamb√©m pertence ao "sklearn". 

#### 2.3 - Gerando um gr√°fico Dendrograma (Hier√°rquico) para explora√ß√£o da distribui√ß√£o
Aqui vamos usar a biblioteca "scipy" para gerar um mapa hierarquico da distribui√ß√£o dos valores das features. 

![Dendograma](assets/2_3_a.png)

J√° nesta an√°lise conseguimos ver claramente ao menos 3 grandes grupos. 

#### 2.4 - Aplicar m√©todo cotovelo para identificar o n√∫mero de Clusters
A ideia desta etapa √© gerar o gr√°fico de cotovelo para poder identificar o n√∫mero ideal de clusters. 

![Metodo Cotovelo](assets/2_4_a.png)

Como podemos ver, a curva se mantem firme at√© o final por√©m tem uma queda significativa na descida entre o 3 e 4, ou seja, recomenda o uso de 8 clusters por√©m tamb√©m corrobora o uso de 3 cluster como apontado pelo Demor√°fico.

#### 2.5 - Treinando o modelo e executando ele
Vamos usar o valor 3 identificado pelo demografico, tamb√©m corroborado pelo m√©todo de cotovelo que mostra uma queda na intensidade de descida a partir do mesmo n√∫mero. O valor 3 ser√° usado como o n√∫mero ideal para o n√∫mero de clusters e medir o resultado com o silhoutte score da uma nota ao processo de clusteriza√ß√£o pela coes√£o e separa√ß√£o. 
Nesta medi√ß√£o quanto menor a distancia m√©dia entre os pontos maior √© a coes√£o (e melhor √© este cluster)
Quanto maior a dist√¢ncia entre os clusters diferentes, melhor ter√° sido a separa√ß√£o entre eles. 

Vamos exibir esta distribui√ß√£o em 2D, para isso vamos usar a tecnica de PCA para reduzir a dimensionalidade. 

![KMeans](assets/2_5_a.png)

#### 2.6 - Conclus√£o
Pela distribui√ß√£o dos dados percebemos 3 grandes grupos bem distribuidos onde:
- Grupo mais a esquerda mostra baixa produtividade em condi√ß√µes clim√°ticas baixas (no m√°ximo mediana)
- Grupo mais ao centro e mais alto, onde mostra que o equilibrio das condi√ß√µes clim√°ticas gera uma maior produtividade
- Grupo mais a direita onde condi√ß√µes mais extremas do clima compromete novamente a produtividade.

Em resdumo Muita √°gua, humidade e temperatura compromente a produtividade da safra da mesma forma que pouca √°gua, humidade e temperatura tamb√©m. 

### 3 - Treinar 5 modelos (usando algoritmos diferentes) para predizer o rendimento da safra
- Escolher 5 modelos diferentes
- treinar os modelos
- comparar os resultados de cada modelo 
- testar com pelo menos um modelo a explicabilidade usando LIME e SHAP

#### 3.1 - Escolher os modelos
Escolhemos os 5 modelos abaixo para este experimento:
- LinearRegression
- RidgeCV
- SVR_RBF
- RandomForest
- GradientBoosting

#### 3.2 - Separar os dados de teste e treinamento, e treinar o modelo
Nesta etapa vamos sepaarar parte dos dados para treino e parte para testar o modelo depois. Vamos usar o m√©todo de K-Fold que separar os dados em v√°rios peda√ßos e treina e valida o resultado do modelo, depois re-treina com uma nova por√ß√£o dos dados e valida com outra, assim por diante at√© ter usado dos os dados como teste e como massa para treino. Feito √© c√°lculado a m√©dia do resultado aferido em cada um destes cen√°rios. 

#### 3.3 - Avaliar o resultado obitdo com cada modelo
Os modelos foram avaliados usando as m√©tricas: 
- R2 (Coeficiencia de Determinacao) -> esta m√©trica quanto mais perto de 1 indica que o mdelos √© mais preciso, quanto mais longe, menos preciso
- MAE (Mean Absolute Error / Erro Absoluto M√©dio) -> M√©dia das diferen√ßas em valor absoluto entre o real e o previsto
- RMSE (Root Mean Squared Error / Raiz do Erro Quadr√°tico M√©dio) -> √â a raiz quadrada da m√©dia dos erros ao quadrado:

![Resultado](assets/3_3_a.png)

Em resumo o modelo baseado em GradientBoosting teve a melhor performance entre os 5 testados. 

### 4 - Estimativa de custos para executar estes modelos
Nesta estapa vamos usar a calculadora da AWS para estimar os custos mensal da infraestrutura para executar estes modelos

#### 4.1 - Requisitos
A infraestrutura necess√°ria para executar estes c√≥digos em Python √©:
1 Servidor Linux com as seguintes caracter√≠sticas:
- 2 CPUs.
- 1 GIB de mem√≥ria.
- At√© 5 Gigabit de rede.
- 50 GB de armazenamento (HD).

#### 4.2 - Usando a calculadora AWS
Acessa a URL a seguite para abrir a calculadora AWS, chmada de "AWS Pricing calculator".
<a href="https://calculator.aws/#/">Calculadora AWS</a>

Siga os passos abaixo:
a) Clique no bot√£o "Criar uma estimativa"
![Criar uma estimativa](assets/4_2_a.png)

b) Escolha "Pesquisa por tipo de Local", mantenha a op√ß√£o "Estolher um tipo de local" como "Regi√£o", e no campo "Escolher Regi√£o" escolha a op√ß√£o: "sa-east-1".

Esta regi√£o √© a √∫nica da AWS dispon√≠vel no Brasil, para projetos em por quest√µes de legisla√ß√£o, como a LDGP, onde os dados n√£o podem sair do pais, esta √© a op√ß√£o que mantem voc√™ aderente as estas restri√ß√µes.

![Escolha da regi√£o](assets/4_2_b.png)

c) Preencha "ec2" no campo de busca "Localizar servi√ßo", at√© que apare√ßa na lista abaixo o servi√ßo "Amazon EC2" que √© o servi√ßo de servidores virtuais da Amazon, e clique em "Configurar".

![Escolhendo o servi√ßo](assets/4_2_c.png)

d) Preencha os campos abaixo:
- Loca√ß√£o: "instancias compartilhadas"
- Sistema operacional: "Linux"
- Cargas de trabalho: "Uso constante"
- N√∫mero de Inst√¢ncias: "1"
- Pesquisa tipo de Instancia: "t3.micro"

Feito isso voc√™ ver√° que na lista de instancias aparecer√° somente uma inst√¢ncia e ela tem os requisitos que precisamos, selecinoe ela na lista e confirme no bot√£o "Salvar e adicionar servi√ßo"

![Adicionando o servidor na conta√ß√£o](assets/4_2_d.png)

e) Confirme o valor do or√ßamento clicando no bot√£o "Visualizar Resumo"

![Finalizando a cota√ß√£o](assets/4_2_e.png)

Com isso a calculadora vai mostrar um resumo dos seus custos, que para o meu usu√°rio, no momento da realiza√ß√£o desta cota√ß√£o, ficou em US$ 58,68 por ano j√° incluindo um custo inicial. 

![Resumo final](assets/4_2_f.png)

Segue o link para o video da explica√ß√£o do uso da calculadora AWS: 
<a href="https://youtu.be/fE5j5tk_hIk">Video no Youtube</a>

## üìÅ Estrutura de pastas

Dentre os arquivos e pastas presentes na raiz do projeto, definem-se:

- <b>assets</b>: aqui est√£o os arquivos relacionados a elementos n√£o-estruturados deste reposit√≥rio, como imagens.

- <b>jupyter</b>: aqui foi armazendo o Jupyter Notebook usado para explorar os dados, treinar os modelos e fazer os experimentos

- <b>README.md</b>: arquivo que serve como guia e explica√ß√£o geral sobre o projeto (o mesmo que voc√™ est√° lendo agora).


## üìã Licen√ßa

<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://github.com/agodoi/template">MODELO GIT FIAP</a> por <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://fiap.com.br">Fiap</a> est√° licenciado sobre <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">Attribution 4.0 International</a>.</p>
